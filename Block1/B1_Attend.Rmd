---
title: "Partial/Marginal effects and their standard errors"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(margins)
library(car)
library(RcmdrMisc)
```

***

# Introduction

### In this example, we deal with effects of attendance on final exam performance

A regression model is used to explain the outcome on a final exam (`stndfnl`, standardized observations) as a function of the following regressors: 

* percentage of classes attended, 
* prior college grade point average, 
* ACT score (American College Testing).

### Data: 
```{r}
attend <- load("attend.RData")
print(desc)
``` 

***

### Base model (no interactions)

$$\textit{stndfnl}_{\,i} = \beta_0 + \beta_1 \textit{atndrte}_i
+ \beta_2 \textit{priGPA}_i + \beta_3 \textit{ACT}_i + u_i$$


```{r}
lm1<-lm(stndfnl ~ atndrte + priGPA + ACT, data=data)
summary(lm1)
```

The above standard errors come from the OLS estimation of variance-covariance matrix of estimated parameters:
$$
\textit{var}(\hat{\pmb{\beta}})=\hat{\sigma}^2(\pmb{X}^{\prime}\pmb{X})^{-1}.
$$  
Different types of robust standard errors may be calculated, White's HC0 are given by:
$$
\textit{var}(\hat{\pmb{\beta}})= 
\pmb{\hat{\,\Sigma \,}} = [(\pmb{X}^{\prime}\pmb{X})^{-1}\pmb{X}^{\prime}]
\pmb{\hat{\,\Omega \,}}
[(\pmb{X}^{\prime}\pmb{X})^{-1}\pmb{X}^{\prime}]^{\prime}
$$
where $\pmb{\hat{\Omega}}$ is a diagonal matrix with $(i,i)$ elements equal to $\hat{u}_i^2$.

```{r}
coeftest(lm1, vcov = vcovHC(lm1, "HC0")) # White's HC0 robust errors
``` 

Here, given the linear nature of the model, the estimated $\hat{\beta}_1$ to $\hat{\beta}_3$ coefficients are:

* the ceteris paribus marginal effects. They are valid regardless of magnitudes of their corresponding regressors or magnitudes of other regressors.  
    * Can be viewed as Average partial effects, APEs (average marginal effects)
    * Can be viewed as Partial effects at the average, PEAs (marginal effects at the average)

Standard errors $\textit{s.e.}(\hat{\beta}_j)$ are valid, regardless of observed regressor values (magnitudes).


***

### Model with interaction term: `atndrte * priGPA`

This LRM may be outlined as:

$$\textit{stndfnl}_{\,i} = \beta_0 + \beta_1 \textit{atndrte}_i
+ \beta_2 \textit{priGPA}_i + \beta_3 \textit{ACT}_i  
+ \beta_4 (\textit{atndrte}_i \times \textit{priGPA}_i  ) + u_i$$


```{r}
lm2<-lm(stndfnl ~ atndrte + priGPA + ACT + atndrte:priGPA , data=data)
summary(lm2)
``` 


Now, the partial effects of `atndrte`, `priGPA` and `ACT` are as follows:

* $\frac{\partial \textit{stndfnl}}{\partial \textit{atndrte}_i} = \beta_1 + \beta_4 \textit{priGPA}_i$ 

* $\frac{\partial \textit{stndfnl}}{\partial \textit{priGPA}_i} = \beta_2 + \beta_4 \textit{atndrte}_i$ 

* $\frac{\partial \textit{stndfnl}}{\partial \textit{ACT}} = \beta_3$ 
  

#### Partial effects:  

* $\textit{ACT}$: Interpretation of $\beta_3$ does not change from previous example (no interaction).

* $\textit{atndrte}$: The partial effect on the dependent variable with respect to $\textit{atndrte}$ depends on the magnitude of $\textit{priGPA}$. 
    - $\beta_1$ and $\textit{s.e.}(\hat{\beta}_1)$ are valid (directly applicable) only if $\textit{priGPA} =0$. Generally, this is not very useful.
    - For any $\textit{priGPA} \neq 0$, the partial effect of $\textit{atndrte}$ is easy to calculate. However, standard error of the $\textit{atndrte}$ partial effect is not directly available here. 
    - Reparametrization and Delta method may be used to get the s.e. (Bootstrap as well).



* $\textit{priGPA}$: same situation as above, partial effects depend on the magnitude of the interacting element.


***

### Marginal effects - Reparametrization 

Say, we are interested in partial effects and standard errors for a representative student with average `atndrte` and `priGPA` values. The effects can be calculated easily:

```{r}
m1 <- mean(data$atndrte)
m2 <- mean(data$priGPA)
m1 # mean(data$atndrte)
m2 # mean(data$priGPA)
``` 
 

* $\frac{\Delta \textit{stndfnl}}{\Delta \textit{atndrte}_i} \doteq -0.021 + 0.011 \, \overline{\textit{priGPA}} = -0.021 + 0.011 \times 2.587 = 0.009$


* $\frac{\Delta \textit{stndfnl}}{\Delta \textit{priGPA}_i} \doteq - 0.555 + 0.011 \, \overline{\textit{atndrte}} = - 0.555 + 0.011 \times 81.710 = 0.382$

* Standard errors to these effects are available through reparametrization of the interaction term:
  $([\textit{atndrte}_i - \overline{\textit{atndrte}} ] \times [\textit{priGPA}_i - \overline{\textit{priGPA}} ] )$

* In the reparametrized model, $\beta_1$ is the effect of unit change in $\textit{atndrte}$ while $\textit{priGPA} =  \overline{\textit{priGPA}}$

* $\beta_2$ can be interpreted by analogy (in the reparametrized model).

```{r}
lm3<-lm(stndfnl ~ atndrte + priGPA + ACT + I((atndrte-m1)*(priGPA-m2)) , data=data)
summary(lm3)
``` 



***

### Marginal effects using `{margins}` package

Again, we are interested in partial effects and standard errors for a representative student with average `atndrte` and `priGPA` values. The effects can be calculated easily through `{margins}` package

```{r}
lm2<-lm(stndfnl ~ atndrte + priGPA + ACT + atndrte:priGPA , data=data) # repeated
margins(lm2, at = list(atndrte = m1, priGPA = m2)) # see help for {margins} package
``` 

Standard errors are not yet implemented in `{margins}` for the `at=` argument. 

---


### Delta method - marginal effects & standard errors. 


**Delta method - short description**

- Used to estimate (approximate) the variance of a non-linear function $g$ of coefficient estimates $\hat{\pmb{\beta}}$.  
- Approximation of variance is based on first order Taylor series expansion.  

$$
\text{var} \left[  g(\hat{\pmb{\beta}}) \right] 
\approx  \nabla^{\prime} g(\hat{\pmb{\beta}}) \, \cdot \,
\text{var}(\hat{\pmb{\beta}}) \, \cdot \, \nabla g(\hat{\pmb{\beta}})
$$

- while the gradient $\nabla g(\hat{\pmb{\beta}})$ can often be obtained analytically, `R` would use a numerical approximation (not discussed here).  


--- 



**Delta method - a more detailed intuition**  

Consider a vector of estimated parameters $\hat{\pmb{\beta}}$ and some function $\hat{{g}} = f(\hat{\pmb{\beta}})$. Both $\hat{\pmb{\beta}}$ and $g$ are stochastic and the variance of $\hat{g}$ is a function of variance of $\hat{\pmb{\beta}}$. 

1. If $g$ is a linear function of $\pmb{\beta}$, say $g = \pmb{\omega}^{\prime}\pmb{\beta}$ then: $$ \text{var} (\hat{g}) = \pmb{\omega}^{\prime} \text{var}(\hat{\pmb{\beta}})\, \pmb{\omega}$$
(recall how variance of fitted value $\hat{y}_i$ is calculated).  

2. If $g$ is non-linear, variance of $\hat{g}$ can be approximated through a first-order Taylor expansion. 

---

Specifically, if $g$ is a function of parameters $\pmb{\beta}$, and if $\hat{\pmb{\beta}}$ is a consistent, normally distributed estimator for that parameter:
$$
g(\hat{\pmb{\beta}}) \approx g(\pmb{\beta}) + \nabla g(\pmb{\beta})^\prime (\hat{\pmb{\beta}} - \pmb{\beta})
$$
Since $\pmb{\beta}$ is a constant, and $\hat{\pmb{\beta}}$ is a consistent estimator for $\pmb{\beta}$, we can then say:
$$
\sqrt{n}\left(g(\hat{\pmb{\beta}})-g(\pmb{\beta})\right)\,\xrightarrow{D}\,N\left(0, \nabla g(\pmb{\beta})^\prime \cdot \pmb{\Sigma}_{\hat{\pmb{\beta}}} \cdot \nabla g(\pmb{\beta})\right)
$$


where $\hat{\pmb{\beta}}$ and $\Sigma_{\hat{\pmb{\beta}}}$ come from the OLS estimate and $g(\pmb{\beta})$ is the marginal effect.   


--- 

--- 

#### Emprical example 1, using `attend` data

In this exmple, we consider a *simple* Delta method example, as we only deal with linear functions of $\beta$.    

For our linear model,  

$$\textit{stndfnl}_{\,i} = \beta_0 + \beta_1 \textit{atndrte}_i
+ \beta_2 \textit{priGPA}_i + \beta_3 \textit{ACT}_i  
+ \beta_4 (\textit{atndrte}_i \times \textit{priGPA}_i  ) + u_i,$$



Let's consider a unit change in `atndrte` (and fixing $\textit{priGPA}=\overline{\textit{priGPA}}$):  

* Marginal effect can be expressed as a linear function (linear in parameters): 
$$
g(\pmb{\beta}) = \beta_1 + \beta_4 \overline{\textit{priGPA}}
$$

* If you take the gradient of this function (remember, a function of relevant *coefficients* not of regressors), it would be:
$$
\nabla g(\pmb{\beta})^\prime = [1,\,\, \overline{\textit{priGPA}}]^{\,\prime} \,,
$$

* The matrix $\Sigma_{\hat{\pmb{\beta}}}$ comes from the OLS estimation of our main regression:

$$
\Sigma_{\hat{\pmb{\beta}}} = 
\left[
\begin{matrix}
\textit{var}(\hat{\beta}_1) & \textit{cov}(\hat{\beta}_1,\hat{\beta}_4) \\
\textit{cov}(\hat{\beta}_1,\hat{\beta}_4) & \textit{var}(\hat{\beta}_4)
\end{matrix}\right]
$$


* Combining the above, the variance of $\hat{g} \equiv g(\hat{\pmb{\beta}})$ can be calculated using $\textit{var} (g) = \nabla g(\pmb{\beta})^\prime \cdot \pmb{\Sigma}_{\hat{\pmb{\beta}}} \cdot \nabla g(\pmb{\beta})$, i.e. as:
$$
\textit{var}(\hat{g}) =
[1,\,\, \overline{\textit{priGPA}}]
\left[
\begin{matrix}
\textit{var}(\hat{\beta}_1) & \textit{cov}(\hat{\beta}_1,\hat{\beta}_4) \\
\textit{cov}(\hat{\beta}_1,\hat{\beta}_4) & \textit{var}(\hat{\beta}_4)
\end{matrix}\right]
\left[
\begin{matrix}
1 \\
\overline{\textit{priGPA}}
\end{matrix}\right]
$$


Marginal effect & statistical inference of a unit change in `atndrte` (fixing priGPA at mean) can be calculated as follows:

```{r}
V <- vcov(lm2)
V # Full VCV matrix for beta coeffs of our LM
V1 <- V[c(2,5),c(2,5)]
V1 # VCV extracted for beta1 and beta4
g1 <- c(1, m2) # gradient
g1 # show gradient (as row vector)
VC1 <- t(g1) %*% V1 %*% g1
sqrt(VC1) # s.e. of the marginal effect through delta metod. Compare to "Reparametrization"
```


---

#### Emprical example 2, using `attend` data

For a unit change in `priGPA` (and fixing $\textit{atndrte}=\overline{\textit{atndrte}}$):

```{r}
V <- vcov(lm2) # Full VCV, (repeated for clarity)
V2 <- V[c(3,5),c(3,5)]  # (Sigma matrix for beta2 and beta4)
g2 <- c(1, m1) # gradient
VC2 <- t(g2) %*% V2 %*% g2
sqrt(VC2) # compare to "Reparametrization"
```


--- 

#### Emprical example 3, using `attend` data


The previous two steps can be combined as follows:

Let's define a matrix $\pmb{G}=\{\partial g_i(\pmb{\beta}) / \partial(\beta_j)\}$

In our case (for marginal effects of the two interacting regressors): 

$$
\pmb{G}=
\left[
\begin{matrix}
1 & 0 & \overline{\textit{priGPA}} \\
0 & 1 & \overline{\textit{atndrte}}
\end{matrix}\right]
$$

Now, the variance of marginal effects can be estimated as 
$$
\textit{var}[g(\pmb{\hat{\beta}})] = \pmb{G} \,\,\pmb{\Sigma}_{\hat{\pmb{\beta}}} \, \pmb{G}^{\prime} \,,
$$
```{r}
V <- vcov(lm2) # repeated
V3 <- V[c(2,3,5),c(2,3,5)]
G <- matrix(c(1,0,m2,0,1,m1), nrow=2,byrow = T)
VCV <- G %*% V3 %*% t(G)
sqrt(diag(VCV)) # compare to "Reparametrization"
```




***

#### Delta method using the `{car}` and `{RcmdrMisc}` packages

(Please note that `DeltaMethod()` in `{RcmdrMisc}` is a wrapper i.e. inteface to `deltaMethod()` from `{car}`)

1) Effect of $\textit{atndrte}$ at $\textit{priGPA} = \overline{\textit{priGPA}}$: 

   `DeltaMethod()` syntax is based on the first derivative of the regression function

$$
\textit{stndfnl}_{\,i} = \beta_0 + \beta_1 \textit{atndrte}_i
+ \beta_2 \textit{priGPA}_i + \beta_3 \textit{ACT}_i  
+ \beta_4 (\textit{atndrte}_i \times \textit{priGPA}_i  ) + u_i
$$

  with respect to $\textit{atndrte}$:

```{r}
DeltaMethod(lm2,"(b1+b4*m2)") # m2 is the average value of priGPA
# note how the coefficient numbering follows from lm() formula:
# lm2 <- lm(stndfnl ~ atndrte + priGPA + ACT + atndrte:priGPA , data=data)
``` 

2) For the effect of $\textit{priGPA}$ at $\textit{atndrte} = \overline{\textit{atndrte}}$:

```{r}
DeltaMethod(lm2,"(b2+b4*m1)")# m1 is the average value of atndrte
``` 


***

### APEs/AMEs using the `{margins}` package

Here, we use our sample to estimate (population) the average/expected effect of changing a regressor by 1 unit (given observed covariates):

$\textit{APE}_j = \frac{1}{n} \sum_{i=1}^{n} f_j(\boldsymbol{x}_i^{\prime} \boldsymbol{\hat{\beta}})$

where $f_j(.) = \frac{\partial \hat{y}}{\partial x_j}$ : is the first derivative of the fitted RHS of regression equation along the $x_j$ regressor.

```{r}
lm2<-lm(stndfnl ~ atndrte + priGPA + ACT + atndrte:priGPA , data=data) # repeated
summary(margins(lm2))
```


***

***

# Supervised work:

#### 1) We expand the `lm2` model to include $\textit{priGPA}^2$ and $\textit{ACT}^2$ as regressors:

$$\textit{stndfnl}_{\,i} = \beta_0 + \beta_1 \textit{atndrte}_i
+ \beta_2 \textit{priGPA}_i + \beta_3 \textit{ACT}_i  
+ \beta_4 \textit{priGPA}_i^2 + \beta_5 \textit{ACT}_i^2 
+ \beta_6 (\textit{atndrte}_i \times \textit{priGPA}_i  ) 
+ u_i$$

##### Estimate the model

```{r}
# Fill-in and uncomment the following lines
# lm4 <-lm(stndfnl ~ ... , data=data) 
# summary(lm4)
```

##### Test $H_0: \beta_1 = \beta_6 = 0$
```{r}
# Fill-in and uncomment the following lines
# ?linearHypothesis
# linearHypothesis(lm4, ... )
```

For results verification, see Wooldridge, Introductory econometrics, Example 6.3


#### 2) Use R (`DeltaMethod`) to calculate marginal effects and standard errors. Use mean values of all regressors as your "interesting" observations.

```{r}
# 
m3 <- mean(data$ACT)
# 
# Effect and s.e. of atndrte
#
# Effect and s.e. of priGPA
#
# Effect and s.e. of ACT
#
```

#### 3) Use R (`margins`) to calculate APEs of all regressors, interpret


```{r}
# APEs 
# 
```

***


For additional examples, see `margins`: [https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html).

